init de GoLang
go mod init llm-rag-go
go get github.com/gin-gonic/gin  # Framework HTTP
go get github.com/sashabaranov/go-openai
go get github.com/tmc/langchaingo  # Librairie Go pour RAG et LLM
go get github.com/chroma-core/chroma-go  # Client pour ChromaDB


RAG with LangChain:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   PDF / TXT   â”‚ â—„â”€â”€â”€ Utilisateur charge un document
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PyPDFLoader / TXT  â”‚ â—„â”€â”€â”€ Extraction du contenu texte
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Text Splitter (chunks)     â”‚ â—„â”€â”€â”€ DÃ©coupe en morceaux gÃ©rables
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Sentence Transformers (Embeddings)   â”‚ â—„â”€â”€â”€ Transforme texte â†’ vecteurs
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ChromaDB (vector store)    â”‚ â—„â”€â”€â”€ Indexation + stockage des vecteurs
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Question posÃ©e par l'user   â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Embedding de la question (vecteur)       â”‚
â”‚ + recherche des chunks similaires        â”‚ â—„â”€â”€â”€ via `retriever = vectorstore.as_retriever()`
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ChatOpenAI / LLM                   â”‚ â—„â”€â”€â”€ Le modÃ¨le gÃ©nÃ¨re une rÃ©ponse
â”‚ + contexte = rÃ©ponse pertinente    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜


utilisation de LangChain:
âœ‚ï¸ 2. from langchain.text_splitter import RecursiveCharacterTextSplitter

â¤ Ce que Ã§a fait :
Coupe un document long en petits morceaux (chunks) pour pouvoir :

les indexer proprement dans la base vectorielle
respecter la limite de tokens des modÃ¨les de langage
ğŸ“¦ Exemple :
splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
chunks = splitter.split_documents(documents)
â¡ï¸ Chaque chunk fait environ 500 caractÃ¨res, et chevauche de 50 pour garder du contexte entre eux.

ğŸ”¤ 3. from langchain.embeddings import HuggingFaceEmbeddings

â¤ Ce que Ã§a fait :
Transforme chaque chunk de texte en vecteur numÃ©rique (embedding). Ces vecteurs sont ensuite stockÃ©s dans Chroma et utilisÃ©s pour retrouver les passages les plus pertinents.

ğŸ“¦ Exemple :
embedding_model = HuggingFaceEmbeddings(
    model_name="sentence-transformers/all-MiniLM-L6-v2"
)
â¡ï¸ Ce modÃ¨le encode chaque texte en vecteurs de 384 dimensions.

ğŸ§  4. from langchain.vectorstores import Chroma

â¤ Ce que Ã§a fait :
Permet de stocker les vecteurs gÃ©nÃ©rÃ©s prÃ©cÃ©demment, de faÃ§on Ã  pouvoir ensuite faire une recherche sÃ©mantique rapide.

ğŸ“¦ Exemple :
vectordb = Chroma(
    collection_name="rag_collection",
    embedding_function=embedding_model,
    client_settings=chroma_client_settings
)
â¡ï¸ Tu peux ensuite faire :

retriever = vectordb.as_retriever()
pour utiliser cette base dans ta chaÃ®ne RAG.

ğŸ”— 5. from langchain.chains import RetrievalQA

â¤ Ce que Ã§a fait :
CrÃ©e une chaÃ®ne RAG complÃ¨te qui combine :

la recherche des chunks pertinents via retriever
la gÃ©nÃ©ration dâ€™une rÃ©ponse via un LLM (comme GPT)
ğŸ“¦ Exemple :
qa_chain = RetrievalQA.from_chain_type(llm=ChatOpenAI(), retriever=retriever)
â¡ï¸ Ensuite, tu poses une question :

response = qa_chain.run("Quels sont les points clÃ©s du document ?")
ğŸ¤– 6. from langchain.chat_models import ChatOpenAI

â¤ Ce que Ã§a fait :
Permet dâ€™utiliser un modÃ¨le de langage (GPT-3.5, GPT-4, etc.) de faÃ§on propre avec LangChain.

ğŸ“¦ Exemple :
llm = ChatOpenAI(temperature=0)
temperature=0 = rÃ©ponse plus prÃ©cise et dÃ©terministe
tu peux aussi choisir model_name="gpt-4" si tu veux GPT-4